16장 강화학습
=============

#### 행동심리학과 강화학습
강화(Reinforcement)는 동물이 시행착오(Trial and Error)을 통해 학습하는 방법 중 하나로 강화의 핵심은 바로 **보상을 얻게 해주는 행동**의 빈도 증가(예:돌고래 훈련)
#### 기계학습과 강화학습
기계학습은 인공지능의 한 범주로 서 컴퓨터가 스스로 학습하게 하는 알고리즘을 개발하는 연구분야
종류 : 지도학습, 비지도 학습, **강화학습**

### 강화학습(Reinforcement learning)

**행동심리학**에서 영감을 받았으며, 어떤 환경(Envirionment) 안에서 에이전트(Agent)가 현재의 상태(State)를 인식하여 선택 가능한 어떤 행동(Action)을 취한다. 그러면 그 에이전트는 환경으로부터 포상(Reward)을 얻게 된다.(포상은 양수와 음수 둘 다 가능) 강화 학습의 알고리즘은 그 에이전트가 앞으로 누적될 포상을 최대화하는 일련의 행동으로 정의되는 정책(Policy)을 찾는 방법이다. 이러한 문제는 매우 포괄적이기 때문에 게임 이론, 제어이론, 운용 과학, 정보 이론, 시뮬레이션 기반 최적화, 다중 에이전트 시스템, 떼 지능, 통계학, 유전 알고리즘 등의 분야에서도 연구된다.

1. 보상(Reward)을 통해 학습
2. 보상은 컴퓨터가 선택한 행동(Action)에 대한 환경의 반응
3. 따라서 **행동이 결과로 나타나는 보상을 통해 학습**
4. 보상을 얻게하는 행동을 점점 많이 하도록 학습

<img src="https://user-images.githubusercontent.com/4945207/74121687-e7786600-4c0b-11ea-9df7-e0970b33de88.png" width="500" height=""></img>

강화학습의 목적은 결국 **최적의 행동양식 또는 정책**을 학습하는것이다.

### 특징

환경에 대한 사전지식 없이도 학습   
결정을 순차적으로 내려야 하는 문제에 적용(MDP 방법 사용)

### 배경

* 1950년대 부터 시작해 주로 게임과 제어분야에 응용
* 2013년 영국 스타트업 딥마인드 팀이 아타리 게임에 적용한것을 시연하면서 혁명이 일어남
* 2014년 구글이 딥마인드를 5억달러에 인수
* 2017년 알파고가 커제를 이기면서 절정
* 강화학습에 딥러닝을 적용했더니 상상 이상의 성능 발휘함

### 주요개념

* 정책 그레디언트(Policy gradient)
* 심층 Q-네트워크(DQN)
* 마르코프 결정과정(Markov Decision Process : MDP)

## 16.1 보상을 최적화하기 위한 학습

* 소프트웨어 에이전트(Agent)는 주어진 환경(Environment)에서 행동(Action)을 하고 그 결과로 보상(Reward)을 받음
* 에이전트의 목적은 보상의 장기간 **기대치를 최대**로 만드는 행동을 학습하는것
* 양(positive)보상, 음(negative)보상이 있고 시행착오 끝에 합한 보상이 최대가 되게 하는것
   
ex) 보행로봇 제어 프로그램의 경우 환경은 실제세상, 에이전트는 센서를 통해 세상 관찰   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;에이전트가 아타리 제어 프로그램인 경우 환경은 아타리 게임 시뮬레이션이고 액션은 조이스틱 위치
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


## 16.2 정책 탐색

<img width="435" src="https://user-images.githubusercontent.com/4945207/74102427-c3bf0c80-4b86-11ea-8a22-8663ea41e7f6.png"></img>

* 정책(policy) : 에이전트가 행동을 결정하기 위해 사용하는 알고리즘 ex)신경망 정책
* 정책 파라미터 : 정책을 결정하는 변경 가능한 파라미터
* 정책 탐색 : 에이전트가 가장 성능좋은 조합을 고르기 위한 과정   
> 무작위 탐색
> 유전 알고리즘
* 정책 공간 : 정책탐색을 위한 공간
* 관측(obeervation) : 현재 환경(environment)의 상태(state)의 값
* 보상(reward) : 환경으로 받는 보상 값

## 16.3 OpenAI 짐

* 에이전트를 훈련시키기 위한 시뮬레이션 환경
* https://gym.openai.com

### CartPole 환경 시물레이션

카트위에 놓인 막대가 넘어지지 않도록 카트를 왼쪽 또는 오른쪽으로 가속시키는 2D 시뮬레이션


<img src="https://user-images.githubusercontent.com/4945207/74123089-cf571580-4c10-11ea-9a17-bca5bd3fc063.png" width="500" height=""></img>

'''
>>env = gym.make("CartPole-v0")   
>>obs = env.reset()   
>>obs   
>>array([ 0.01592466, -0.02766193, -0.02049984,  0.01750777])   
>> action = 1
>> obs, reward, done, info = env.step(action)
'''
* obs : 관측값
* reward : 보상값, 여기서는 스텝마다 무조건 1
* done : True 이면 에피소드(게임 한판)가 끝난것
* info : 딕셔너리 데이터, 디버깅 정보 등 부가정보가 담길 수 있으므로 훈련에 사용하면 안됨

## 16.4 신경망 정책

* 신경망은 관측을 입력받아 행동을 출력함(행동의 확률을 추정함)
* 점수에 따라 행동하는 것이 아닌 신경망의 확률로 랜덤하게 행동
* 새로운 행동의 **탐험(explore)**와 잘할 수 있는 행동의 **활용(exploit)** 사이의 **균형**을 맞추기 위함
* 그리고 이 환경에서는 과거의 행동과 관측을 무시 => 각 관측(obs)이 환경(env)에 대한 완전한(state)를 담고 있음
* 어떤 상태가 숨겨져 있으면 즉, 예를 들어 환경(env)이 카트의 속도를 제외한 위치정보만 알려준다면 이전 상태에 대한 위치도 알아야 속도를 측정할 수 있음. 이때는 그 이전의 관측도 필요함. 


## 16.5 행동평가 : 신용할당 문제
## 16.6 정책 그래디언트
## 16.7 마르코프 결정 과정

