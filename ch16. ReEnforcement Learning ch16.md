16장 강화학습
=============

#### 기계학습과 강화학습
기계학습은 인공지능의 한 범주로 서 컴퓨터가 스스로 학습하게 하는 알고리즘을 개발하는 연구분야   
종류 : 지도학습, 비지도 학습, **강화학습**

#### 행동심리학
행동심리학에서 강화(Reinforcement)는 동물이 시행착오(Trial and Error)을 통해 학습하는 방법 중 하나로 강화의 핵심은 바로 **보상을 얻게 해주는 행동**의 빈도 증가를 뜻함 (예:돌고래 훈련)

### 강화학습(Reinforcement learning)

**행동심리학**에서 영감을 받았음   
어떤 환경(Envirionment) 안에서 에이전트(Agent)가 현재의 상태(State)를 인식하여 선택 가능한 어떤 행동(Action)을 취한다. 그러면 그 에이전트는 환경으로부터 포상(Reward)을 얻게 된다.(포상은 양수와 음수 둘 다 가능)   
강화 학습의 알고리즘은 그 에이전트가 앞으로 누적될 포상을 최대화하는 일련의 행동으로 정의되는 정책(Policy)을 찾는 방법이다. 이러한 문제는 매우 포괄적이기 때문에 게임 이론, 제어이론, 운용 과학, 정보 이론, 시뮬레이션 기반 최적화, 다중 에이전트 시스템, 통계학, 유전 알고리즘 등의 분야에서도 연구된다.   

정리하자면   
1. 에이전트는 **행동(Action)**을 하고
2. **보상**은 에이전트가 선택한 행동에 대한 환경의 반응
3. 따라서 **행동이 결과로 나타나는 보상(Reward)을 통해 학습**
4. 보상을 얻게하는 행동을 점점 많이 하도록 학습

<img src="https://user-images.githubusercontent.com/4945207/74121687-e7786600-4c0b-11ea-9df7-e0970b33de88.png" width="500" height=""></img>

강화학습의 목적은 결국 보상을 많이 받을 수 있는 **최적의 행동양식 또는 정책**을 학습하는것이다.

### 특징

* 환경에 대한 사전지식 없이도 학습(비지도 학습)   
* 결정을 순차적으로 내려야 하는 문제에 적용됨(MDP 방법)

### 배경

* 1950년대 부터 시작해 주로 게임과 제어분야에 응용
* 2013년 영국 스타트업 딥마인드 팀이 아타리 게임에 적용한것을 시연하면서 혁명이 일어남
* 2014년 구글이 딥마인드를 5억달러에 인수
* 알파고가 2016년 이세돌, 2017년 커제를 이기면서 절정   
&nbsp;&nbsp;&nbsp;(강화학습에 딥러닝을 적용했더니 상상 이상의 성능 발휘)

### 주요개념

* 정책 그레디언트(Policy gradient)
* 심층 Q-네트워크(DQN)
* 마르코프 결정과정(Markov Decision Process : MDP)

## 16.1 보상을 최적화하기 위한 학습

* 소프트웨어 에이전트(Agent)는 주어진 환경(Environment)에서 행동(Action)을 하고 그 결과로 보상(Reward)을 받음
* 에이전트의 목적은 보상의 장기간 **기대치를 최대**로 만드는 행동을 학습하는것
* 양(positive)보상, 음(negative)보상이 있고 시행착오 끝에 합한 보상이 최대가 되게 하는것
   
ex) 보행로봇 제어 프로그램의 경우 환경은 실제세상, 에이전트는 센서를 통해 세상 관찰   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;에이전트가 아타리 제어 프로그램인 경우 환경은 아타리 게임 시뮬레이션이고 액션은 조이스틱 위치
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


## 16.2 정책 탐색

<img width="435" src="https://user-images.githubusercontent.com/4945207/74102427-c3bf0c80-4b86-11ea-8a22-8663ea41e7f6.png"></img>

* 정책(policy) : 에이전트가 **행동(Action)을 결정**하기 위해 사용하는 **알고리즘** ex)신경망 정책
* 정책 파라미터 : 정책을 결정하는 변경 가능한 파라미터
* 정책 탐색 : 에이전트가 가장 성능좋은 조합을 고르기 위한 과정(무작위 탐색, 유전 알고리즘 등)   
* 정책 공간 : 정책 탐색을 위한 상태, 행위, 보상 등으로 구성된 공간
* 관측(obeervation) : 현재 환경(environment)의 상태(state)의 값
* 보상(reward) : 환경으로 받는 보상 값

## 16.3 OpenAI 짐

* 에이전트를 훈련시키기 위한 시뮬레이션 환경
* https://gym.openai.com

### CartPole 환경 시물레이션

카트위에 놓인 막대가 넘어지지 않도록 카트를 왼쪽 또는 오른쪽으로 가속시키는 2D 시뮬레이션


<img src="https://user-images.githubusercontent.com/4945207/74123089-cf571580-4c10-11ea-9a17-bca5bd3fc063.png" width="500" height=""></img>

```python
>>env = gym.make("CartPole-v0")   
>>obs = env.reset()   
>>obs   
>>array([ 0.01592466, -0.02766193, -0.02049984,  0.01750777])   
>>action = 1
>>obs, reward, done, info = env.step(action)
```
* obs : 관측값
* reward : 보상값, 여기서는 스텝마다 무조건 1
* done : True 이면 에피소드(게임 한판)가 끝난것
* info : 딕셔너리 데이터, 디버깅 정보 등 부가정보가 담길 수 있으므로 훈련에 사용하면 안됨

## 16.4 신경망 정책

* 신경망은 관측을 입력받아 행동을 출력함(행동의 확률을 추정함)
* **점수에 따라 행동하는 것이 아닌 신경망에서 나온 확률로 랜덤하게 행동**
* 이것은 새로운 행동의 **탐험(explore)** 과 잘할 수 있는 행동의 **활용(exploit)** 사이의 **균형**을 맞추기 위함
* 그리고 이 환경에서는 과거의 행동과 관측을 무시 => 각 관측(obs)이 환경(env)에 대한 완전한 상태(state)를 담고 있음
* 만약 어떤 상태가 숨겨져 있으면 즉, 환경(env)이 카트의 속도를 제외한 위치정보만 알려준다면 이전 상태에 대한 위치도 알아야 속도를 측정할 수 있음. 이때는 그 이전의 관측도 필요함.

![image](https://user-images.githubusercontent.com/4945207/74226996-dc533200-4d00-11ea-9eaf-d5a7e1ce20b8.png)

## 16.5 행동평가 : 신용할당 문제

* 딥러닝 : 추정된 확률과 타깃 확률 사이의 크로스 엔트로피를 최소화 하여 신경망 훈련
* 강화학습 : 에이전트가 얻을 수 있는 도움은 **보상**뿐, 일반적으로 드물고 지연되어 나타남
  만약 100 step까지 균형 유지 하였다면 100번의 행동 중 어느행동에 보상을 받는지, 받는다면 좋은보상인지 나쁜보상인지 알 수 없음 
* 에이전트가 보상 받았을 때 어떤 행동 덕분인지 알기 어려운 문제를 **신용 할당 문제** 라고 함
* 문제 해결을 위해 행동이 일어난 후 각 단계(step)에 할인계수(γ)를 적용한 보상을 모두 합하여 행동(action)을 평가(행동 = ∑스텝)

![ch16_fig16-6](https://user-images.githubusercontent.com/4945207/74127440-0253d600-4c1e-11ea-9981-5bd1dcd6b1ec.png)

![ch16_fig16-6-2](https://user-images.githubusercontent.com/4945207/74128638-f9183880-4c20-11ea-8a22-fec7b62027ef.png)

* 미래 보상은 현재 보상보다 작고 할인계수가 0에 가까우면 미래 보상이 중요하지 않게 된다. (최초 스텝의 보상이 가치가 큼)
* 보통 할인계수는 0.95 ~0.99 사이다. 할인계수가 0.95인 경우 13스텝 미래 보상은 50%, 0.99 인 경우 63스텝 미래 보상이 50%
* 결론 : **많은 에피소드 실행** 후 모든 행동의 점수를 (평균을 빼고 표준편차로 나누어) 정규화 해야 신뢰할 만한 점수 확보 가능. 그 후 점수가 음수인 경우 나쁘고, 양수인 행동은 좋다고 판단함

## 16.6 정책 그래디언트

* 높은 보상을 얻는 방향의 그래디언트로 정책의 파라미터를 최적화 하는 알로리즘   
1. 신경망 정책이 매 스텝마다 선택된 행동이 더 높은 가능성을 가지도록 만드는 그래디언트 계산(가중치 update 와 유사)
2. 몇 번의 에피소드 실행 후 각 행동의 점수를 계산
3. 행동의 점수가 양수 면 좋은것 의미하므로 미래에 선택될 가능성이 높도록 그래디언트 적용. 점수가 음수이면 반대 그래디언트 적용(각 그래디언트 벡터와 상응하는 행동의 곱)
4. 모든 결과 그래디언트 벡터를 평균내어 경사 하강법 스텝 수행

### 실행
1. 알고리즘이 정책을 실행
2. 매 스텝마다 이 그래디언트 텐서를 평가 후 그 값을 저장
3. 여러번의 에피소드가 끝난 후 그래디언트를 업데이트
4. 업데이트 된 그레디언트의 평균을 구함
5. 최적화 단계 수행을 위해 업데이트 된 그래디언트를 옵티마이저에 넣고(그래디언트 벡터마다 플레이스 홀더 필요)
6. 업데이트 된 그래디언트 적용 연산 필요

책의 예제에서는 1000번의 스텝을 가지고 있는 에피소드가 10번의 실행한 후 행동점수를 계산하고   
그다음 훈련될 변수 하나하나에 대해 모든 에피소드와 모든 스텝에 걸쳐 그래디언트 벡터와 행동점수를 곱함   
그리고 계산된 그래디언트의 평균을 계산하여 훈련되는 변수마다 하나씩 주입하여 훈련연산을 수행한 후 모델을 저장함   
알파고의 경우도 이와 유사한 몬테카를로 트리검색을 기반으로 하였음

=>보상의 최적화를 위해 정책을 직접 최적화 함

## 16.7 마르코프 결정 과정 - 기초이론

### 마르코프 연쇄(Markov Chain)
* 메모리가 없는 확률과정(stochastic process)
* 정해진 갯수의 상태를 가짐
* 다른 상태로 주어진 확률에 따라 랜덤하게 전이됨(s -> s'), 단 **전이를 위한 확률은 고정**
* 과거 상태와 상관없이 (s, s')쌍에만 의존

![그림1](https://user-images.githubusercontent.com/4945207/74139505-d5f88380-4c36-11ea-9bb5-0357e1df9bb4.png)

** 각 상태의 확률합은 1   
** S0 에 남을 확률 : 70%   
** S3 는 나가는 방향이 없기 때문에 영원히 남음(종료상태)

### 마르코프 결정과정(Markov Decision Process:MDP)

* 리처드 벨만 기술(1950년대) : 마르코프 체인과 약간 차이
* 각 스텝에서 에이전트는 **여러 가능한 행동 중 하나를 선택** 할 수 있음(모든 상태에 행동이 하나씩 있고 보상이 동일하다면 마르코프 연쇄임)
* **전이확률은 선택된 행동에 따라 달라짐**
* 어떤 상태로의 전이는 보상(양, 음)을 반환해주는 경우가 있음(보상 없는 전이도 있음)
* 에이전트의 목적은 시간이 지남에 따라 보상을 최대화 하기 위한 정책을 찾는것

![그림3](https://user-images.githubusercontent.com/4945207/74151336-4f03d500-4c4f-11ea-92c4-ec4ae16b4d16.png)

** 3개의 상태와 각 상태별로 독립적 행동(action)을 최대 3개 가짐(각 상태는 행동을 선택할 수 있음)   
** 행동 선택후 다음 상태로 전이될 확률이 랜덤   
** 상태 S0에서 최선은 a0가 틀림 없으나 S1에서는 a0 와 a1 어느것이 나은지 확실치 않음
** 어떤 행동을 어떻게 선택할것인가?

#### 가치함수
* 어떻게 행동할지 결정할 방법(판단의 기준) 필요
* 에이전트가 학습할 수 있도록 문제를 MDP로 정의하면 → MDP를 통해 최적 정책을 찾으면 됨
* 현재 상태에서 앞으로 받을 보상들을 고려해서 선택해야 함
* **현재시간 t로부터 에이전트가 받을 보상의 합 : R(t+1) + R(t+2) + R(t+3) + R(t+4) + R(t+5) + R(t+6) + ...**
   *(보상은 행동을 했을 때가 아닌 그 다음 타임스텝에 받음) R은 확률변수로 정해져 있는 수가 아님(0 or 실수)
* 이때 세가지 문제점 발생

1. 에이전트 입장에서는 지금 받은 보상이나 미래에 받는 보상이나 똑같이 취급합니다.
에이전트는 현재에 보상을 받은 경우와 시간이 좀 흘러서 100의 보상 받은 경우를 구분할 수 없습니다.
감가하지 않았다면 에이전트가 보게 되는 보상의 합은 단순한 덧셈이 되기 때문입니다.

2. 100이라는 보상을 1번 받는 것과 20이라는 보상을 5번 받는 것을 구분할 방법이 없습니다.
사람은 당연하게 구분하는 것을 에이전트는 구분할 수 없습니다.

3. 시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대이고, 1씩 받아도 합이 무한대입니다.
수치적으로 두 경우를 구분할 수 없습니다.

이러한 문제 때문에 에이전트는 단순한 보상의 합으로는 판단을 내리기가 어렵습니다. 따라서 좀더 정확한 판단을 위해 감가율을 고려함
**감가율을 이용한 앞으로 받을 보상의 현재 가치**를 나타내면 다음과 같습니다.

```
R(t+1) + γR(t+2) + γ^2R(t+3) + γ^3R(t+4) + γ^4R(t+5) + ....
```
이 값을 반환값 Gt라고 합시다.

반환값이라는 것은 에이전트가 실제로 환경을 탐험하며 받은 보상의 합입니다.
에이전트가 에피소드가 끝난 후에 '그때로부터 얼마의 보상을 받았지?'라며 보상들을 정산하는 것이 반환값입니다.
만일 에피소드를 t = 1 부터 5까지 진행했다면 에피소드가 끝난 후에 방문했던 상태들에 대한 5개의 반환값이 생길 것입니다.

```
Gt = R(t+1) + γR(t+2) + γ^2R(t+3) + γ^3R(t+4) + γ^4R(t+5) + ....   
G1 = R2 + γR3 + γ^2R4 + γ^3R5 + γ^4R6   
G2 = R3 + γR4 + γ^2R5 + γ^3R6   
G3 = R4 + γR5 + γ^2R6   
G4 = R5 + γR6   
G5 = R6   
```
이러한 반환값은 에피소드가 끝난 후 알수 있음. 
* 정확하지 않더라도 현재의 정보를 통해 보상받을 것을 예측할 수 있음 → **기댓값**

* 가치함수 : 어떠한 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값
  v(s) = E[Gt|St=s] → 반환값의 기댓값으로 표현

https://subsay.tistory.com/13

#### 벨만최적방정식(a.k.a Value function)

* **어떤상태(S)의 최적의 상태가치(State Value) V*(S)** 를 추정하는 방법은 에이전트가 **상태S에 도달한 후 최적으로 행동한다고 가정**하고 **평균적으로 기대**할 수 있는 **할인된 미래 보상의 합** 으로 나타낼 수 있다. 
* 에이전트가 최적으로 행동하면 **벨만 최적 방정식**이 적용됨

![그림7](https://user-images.githubusercontent.com/4945207/74169225-55557980-4c6e-11ea-841c-07493076164d.png)

먼저 모든가치를 0으로 초기화 후 가치반복 알고리즘을 사용해 반복적으로 업데이트 함. 충분한 시간 후 최적의 정책에 대응되는 최적의 상태가치에 수렴함

![그림8](https://user-images.githubusercontent.com/4945207/74169266-643c2c00-4c6e-11ea-8f39-87557d0c58aa.png)

#### Q-가치(Q function)

* 최적의 상태가치를 아는것은 정책을 평가할 때 유용하지만
* **에이전트가 해야할 일을 알려주지 않음**
* 큐가치 Q*(s,a)는 에이전트가 상태 s에 도달해서 행동 a를 선택하고 이 행동의 출력을 얻기전에 평균적으로 기대할 수 있는 할인된 미래 보상의 합. 에이전트가 행동 이후 최적으로 행동할 것이라고 가정함

![그림9](https://user-images.githubusercontent.com/4945207/74169651-fa705200-4c6e-11ea-9e70-0c517549906b.png)

#### 중간정리

* MDP
순차적 행동 결정 문제를 수학적으로 정의한 것이 MDP 입니다.   
MDP는 상태, 행동, 보상, 상태 변환 확률, 감가율, 정책으로 구성돼 있습니다.    
순차적 행동 결정 문제를 푸는 과정은 더 좋은 정책을 찾는 과정입니다.   

* 가치함수
에이전트가 어떤 정책이 더 좋은 정책인지 판단하는 기준이 가치함수이다.   
가치함수는 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합입니다.   
에이전트는 정책을 업데이트할 때 가치함수를 사용할 텐데, 보통 가치함수보다는 에이전트가 선택할 각 행동의 가치를 직접적으로 나타내는 큐함수를 사용합니다.   

* 벨만 방정식
현재 상태의 가치함수와 다음 상태 가치함수의 관계식이 벨만 방정식입니다.   
벨만 기대 방정식은 특정 정책을 따라갓을 때 가치함수 사이의 관계식이다.   
더 좋은 정책을 찾아가다 보면 최적의 정책을 찾을 것입니다.   
최적의 정책은 최적의 가치함수를 받게 하는 정책이며, 그 때 가치함수 사이의 관계씩이 벨만 최적 방정식입니다.

## 16.8 시간차 학습(TD)과 Q-러닝

#### 시간차 학습

* 에이전트가 MDP에 대해 일부 정보만 알고 있을 때를 다룰 수 있음
* 초기에 가능한 상태와 행동만 말고 있고 다른것을 모를 때
* 탐험정책(램덤한 정책 등)을 사용해 MDP를 탐험 후 
* TD학습 알고리즘은 실제로 관측된 전이와 보상에 근거해서 상태 가치의 추정을 업데이트 함
* 한번에 하나의 샘플을 다루는 점에 있어서 경사하강법과 비슷함

![그림10](https://user-images.githubusercontent.com/4945207/74169747-27246980-4c6f-11ea-86ab-34b04c8611d8.png)

각 상태 s에서 **이 에이전트가 이 상태를 떠났을 때 얻을 수 있는 당장의 보상과 나중에 기대할 수 있는 보상을 더한 이동평균**을 저장함

#### Q-러닝 알고리즘

* Q-러닝 알고리즘은 전이확률과 보상을 초기에 알지 못한 상환에서 Q-가치 반복 알고리즘을 적용한 것임

![그림6](https://user-images.githubusercontent.com/4945207/74169772-36a3b280-4c6f-11ea-9ec8-9fa4a5e051d5.png)

* 각 상태-행동(s,a)쌍마다 이 알고리즘이 행동 a를 선택해 상태 s를 떠났을 때 받을 수 있는 보상 r과 나중에 기대할 수있는 보상을 더한 이동편균을 저장함   
* 타깃정책은 최적으로 행동할 것이므로 다음상태에 대한 Q-가치 추정의 최댓값을 선택함   
* 훈련된 ㅣ정책을 실행에 사용하지 않기 때문에 **오프-폴리시** 알고리즘이라고 함

#### 탐험정책

완전한 램덤정책이 모든 상태와 전이를 여러번 경험한다 보장하지만 극단적으로 시간이 오래 걸릴 위험 있음

##### e-그리디 정책

* 각 스템에서 e 확률로 램덤하게 행동하거나 1-e 확률로 그 순간 가장 최선인 것으로 행동(가장 높은 Q-가치)
* 완전랜덤에 비해 이 정책의 장점은 Q-가치 추정이 점점 더 향상되기 때문에 환경에서 관심있는 부분을 살피는 데 점점 더 많은 ㅅ시간 사용
* 다른 방법으로 이전에 많이 시도하지 않았던 행동을 시도하도록 탐험하는 방법 있음

#### 근사 Q-러닝과 딥 Q-러닝(DQN)

* Q-러닝의 문제점은 많은 상태와 행동을 가진 대규모 MDP 적용이 어렵다는 점
* 해결방법은 적절한 개수의 파라미터를 사용하여 Q-가치를 근사하는 **함수 Q쎄타(s,a)** 를 찾는것(근사 Q-러닝)
* Q-가치를 추정하기 위해 사용하는 DNN을 **심층Q-네트워크(DQN)** 이라 함 (→ DQ러닝)

## 16.9 DQN 알고리즘으로 미스 팩맨 플레이 학습하기

* 각자 해볼것
